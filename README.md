# American-Sign-Language Recognition using Deep Learning and Computer Vision
The goal of this project is to build a deep learning model to read the American Sign Language from the images in the dataset from Kaggle. The intention is to create a successful model that will be able to classify the images to the characters in the American Sign Language in a real-time environment.

With the power of computing ability in this advanced computer vision technology, coupled with deep learning models, using sign languages to interpret and communicate with deaf and mute people has become easier. In this repository, different deep learning models are used to interpret different characters in the American Sign Language. 
The objective of this project is to find a mix of different image preprocessing techniques and deep learning architectures after tuning the hyperparameters to interpret the signs in ASL.
The training data is trained from scratch using deep learning architectures as VGG16, Inception_v3 and ResNet50_v2.
In actualizing the proposed future work, the deployment and implementation of two real- time American Sign Language Detection implementations are actualized in a motivation to make the work-packages completed to be transformed for a real-life application aiding the disabled individuals through our community.

The dataset used to solve this problem can be found on [Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet/code). It comprises of two folders, one for the training data and one for testing data. The whole training data comprises of 87,000 images. And it is a collection of 29 folders for the letters A-Z and, ‘del’, ‘space’ and ‘nothing’. Thus, every folder/character has 3000 images. For each image, the size on disk is about 12KB. And the testing data set comprises of 28 folders from A-Z and ‘space’ and ‘nothing’. The test data set contains a mere 28 images, to encourage the use of real- world test images. The whole dataset is of 1.11 GB.
